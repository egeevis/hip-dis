import os
import io
import json
from typing import List, Dict, Any

import streamlit as st

openai_key = st.sidebar.text_input(
    "OpenAI API Key",
    type="password",
    value=os.getenv("OPENAI_API_KEY", st.secrets.get("OPENAI_API_KEY", "")),
    key="openai_api_key_input"
)



# Optional deps (add to requirements.txt):
# streamlit
# python-docx
# PyPDF2
# openai>=1.30.0
# jsonschema

try:
    from docx import Document
except Exception:
    Document = None

try:
    import PyPDF2
except Exception:
    PyPDF2 = None

try:
    from jsonschema import validate as jsonschema_validate
except Exception:
    def jsonschema_validate(instance, schema):
        return True

try:
    from openai import OpenAI
except Exception:
    OpenAI = None

st.set_page_config(page_title="Hiperaktivist ‚Äì Kullanƒ±cƒ± Analiz Sistemi", page_icon="üß©", layout="wide")
st.title("Hiperaktivist ‚Ä¢ Dƒ±≈ü Sistem: Kullanƒ±cƒ± Analiz Motoru")
st.caption("20 soruya verilen yanƒ±tlarƒ±, Eƒüitim i√ßeriƒüi + Teknik & Y√∂ntemler'e sadƒ±k kalarak analiz eder.")

# ------------------------------
# Helpers
# ------------------------------

def read_file(file) -> str:
    name = file.name.lower()
    if name.endswith(".txt") or name.endswith(".md"):
        return file.read().decode("utf-8", errors="ignore")
    if name.endswith(".docx"):
        if not Document:
            return "(python-docx yok ‚Äì requirements'e ekleyin)"
        buf = io.BytesIO(file.read())
        doc = Document(buf)
        return "\n".join([p.text for p in doc.paragraphs])
    if name.endswith(".pdf"):
        if not PyPDF2:
            return "(PyPDF2 yok ‚Äì requirements'e ekleyin)"
        buf = io.BytesIO(file.read())
        reader = PyPDF2.PdfReader(buf)
        pages = []
        for p in reader.pages:
            try:
                pages.append(p.extract_text() or "")
            except Exception:
                pages.append("")
        return "\n".join(pages)
    try:
        return file.read().decode("utf-8", errors="ignore")
    except Exception:
        return ""

ANALYSIS_SCHEMA: Dict[str, Any] = {
    "type": "object",
    "properties": {
        "meta": {
            "type": "object",
            "properties": {
                "education_title": {"type": "string"},
                "num_answers": {"type": "integer"},
                "language": {"type": "string"},
            },
            "required": ["education_title", "num_answers", "language"],
        },
        "themes": {"type": "array", "items": {"type": "string"}},
        "strengths": {"type": "array", "items": {"type": "string"}},
        "growth_areas": {"type": "array", "items": {"type": "string"}},
        "micro_actions": {"type": "array", "items": {"type": "string"}},
        "ga_style_narrative": {"type": "string"},
        "safety_notes": {"type": "string"},
    },
    "required": ["meta", "themes", "strengths", "growth_areas", "micro_actions", "ga_style_narrative"],
}

SYSTEM_PROMPT = """
Sen, Hiperaktivist markasƒ±nƒ±n sunduƒüu ki≈üisel geli≈üim eƒüitimleri i√ßin √∂zel olarak geli≈ütirilmi≈ü Dƒ±≈ü Sistem analiz yapay zek√¢sƒ±sƒ±n.

Amacƒ±n:
- Kullanƒ±cƒ±nƒ±n 20 soruya verdiƒüi yanƒ±tlarƒ±, ilgili eƒüitim i√ßeriƒüi ve GA'nƒ±n Teknik & Y√∂ntemleri doƒürultusunda i≈üleyerek derin, ki≈üisel ve anlamlƒ± bir geli≈üim analizi sunmak.
- √áƒ±ktƒ± tek par√ßa, akƒ±cƒ± ve zengin bir metin olmalƒ±. Madde listeleri yerine, b√ºt√ºnl√ºkl√º bir anlatƒ±m i√ßinde ki≈üisel g√∂zlemler, duygusal farkƒ±ndalƒ±k, eƒüitimden gelen ana fikirler ve uygulanabilir √∂neriler harmanlanmalƒ±.
- Metin GA metodolojisine sadƒ±k, yargƒ±sƒ±z, empatik, g√ºvenli ve profesyonel bir √ºslupta olmalƒ±.
- Nihai hedef, kullanƒ±cƒ±nƒ±n eƒüitimden aldƒ±ƒüƒ± deƒüeri g√ºnl√ºk ya≈üamƒ±na entegre edebilmesini kolayla≈ütƒ±rmaktƒ±r.

Kurallar:
- Kesinlikle ‚Äútemalar, g√º√ßl√º alanlar, geli≈üim alanlarƒ±‚Äù gibi ba≈ülƒ±klar verme.
- Metin, kullanƒ±cƒ± yanƒ±tlarƒ±ndaki ipu√ßlarƒ±nƒ± doƒürudan yansƒ±tsƒ±n, ki≈üiselle≈ütirilmi≈ü hissettirsin.
- Uygulanabilir √∂neriler metnin i√ßine doƒüal bi√ßimde yedirilsin.
- Gerekiyorsa g√ºvenlik / kriz uyarƒ±larƒ±nƒ± metnin sonunda ekle.
""".strip()


USER_TEMPLATE = """
# Eƒûƒ∞Tƒ∞M √ñZETƒ∞
{education_summary}

# TEKNƒ∞K & Y√ñNTEMLER √ñZETƒ∞
{techniques_summary}

# SORULAR
{questions_json}

# KULLANICI YANITLARI
{answers_json}

# JSON ≈ûEMA
{json_schema}
""".strip()

# ------------------------------
# Sidebar settings
# ------------------------------
st.sidebar.header("Ayarlar")
openai_key = st.sidebar.text_input(
    "OpenAI API Key",
    type="password",
    value=os.getenv("OPENAI_API_KEY", st.secrets.get("OPENAI_API_KEY", "")),
    key="openai_api_key_sidebar"
)
model = st.sidebar.text_input("Model", value="gpt-4o-mini")
language = st.sidebar.selectbox("Dil", ["T√ºrk√ße", "English"], index=0)
max_actions = st.sidebar.slider("Mikro eylem sayƒ±sƒ±", 3, 10, 5)
temperature = st.sidebar.slider("Temperature", 0.0, 1.0, 0.3, 0.05)

client = None
if openai_key and OpenAI:
    try:
        client = OpenAI(api_key=openai_key)
    except Exception as e:
        st.sidebar.error(f"OpenAI istemcisi ba≈ülatƒ±lamadƒ±: {e}")
# ------------------------------
# Inputs
# ------------------------------
left, right = st.columns(2)
with left:
    q_file = st.file_uploader("Soru Seti (JSON)", type=["json"], key="qjson")
with right:
    edu_file = st.file_uploader("Eƒüitim Dosyasƒ± (docx/pdf/txt/md)", type=["docx", "pdf", "txt", "md"], key="edu")

ty_file = st.file_uploader("Teknik & Y√∂ntemler (docx/pdf/txt/md)", type=["docx", "pdf", "txt", "md"], key="ty")

questions = []
q_meta = {}
if q_file:
    try:
        raw = json.loads(q_file.read().decode("utf-8"))
        questions = raw.get("questions", [])
        q_meta = raw.get("meta", {})
    except Exception as e:
        st.error(f"Soru JSON okunamadƒ±: {e}")

# Render dynamic form
st.markdown("---")
st.subheader("üìù Kullanƒ±cƒ± Yanƒ±tlarƒ±")
answers: List[Dict[str, Any]] = []
if questions:
    for i, q in enumerate(questions, start=1):
        qid = q.get("id", str(i))
        label = q.get("question", f"Soru {i}")
        ans = st.text_area(label, key=f"ans_{qid}", height=120)
        answers.append({"id": qid, "answer": ans})
else:
    st.info("L√ºtfen soru seti JSON'unu y√ºkleyin (√∂r. sorular_1.json).")

# Show previews for context docs
edu_text = ""
tech_text = ""
if edu_file:
    edu_text = read_file(edu_file)
    with st.expander("Eƒüitim Metni (√∂nizleme)", expanded=False):
        st.text_area("Eƒüitim metni √∂nizleme", value=edu_text[:6000], height=200, label_visibility="collapsed")
if ty_file:
    tech_text = read_file(ty_file)
    with st.expander("Teknik & Y√∂ntemler (√∂nizleme)", expanded=False):
        st.text_area("Teknik & Y√∂ntemler √∂nizleme", value=tech_text[:6000], height=200, label_visibility="collapsed")

# ------------------------------
# LLM helpers
# ------------------------------

def summarize_text(client, model: str, text: str, label: str) -> str:
    prompt = f"Metni 10-12 maddeyle kƒ±sa, √∂z ve bilgi kaybƒ± olmadan √∂zetle. Ba≈ülƒ±k: {label}.\n\nMetin:\n{text[:12000]}"
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "Kƒ±sa ve bilgi kaybƒ± olmadan √∂zetleyen bir yardƒ±mcƒ± yazarsƒ±n."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"(√ñzetlenemedi: {e})"


def generate_analysis(client, model: str, system_prompt: str, user_prompt: str, temperature: float = 0.3) -> Dict[str, Any]:
    resp = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=temperature,
        response_format={"type": "json_object"},
    )
    content = resp.choices[0].message.content
    try:
        data = json.loads(content)
    except Exception:
        data = {"raw": content}
    return data

# ------------------------------
# Generate
# ------------------------------
c1, c2, c3 = st.columns([1.2,1,1])
with c1:
    can_generate = client and questions and any(a.get("answer") for a in answers) and (edu_text and tech_text)
    if st.button("üß† Analizi √úret", type="primary", use_container_width=True, disabled=not can_generate):
        with st.spinner("Analiz hazƒ±rlanƒ±yor‚Ä¶"):
            edu_summary = summarize_text(client, model, edu_text, "Eƒüitim √ñzeti")
            ty_summary = summarize_text(client, model, tech_text, "Teknik & Y√∂ntemler √ñzeti")

            # Trim micro action count in schema hint (model uses it in narrative)
            schema = ANALYSIS_SCHEMA.copy()

            system_prompt = SYSTEM_PROMPT
            user_prompt = USER_TEMPLATE.format(
                education_summary=edu_summary,
                techniques_summary=ty_summary,
                questions_json=json.dumps(questions, ensure_ascii=False),
                answers_json=json.dumps(answers, ensure_ascii=False),
                json_schema=json.dumps(schema, ensure_ascii=False),
            )

            data = generate_analysis(client, model, system_prompt, user_prompt, temperature)

            # Attach meta
            data.setdefault("meta", {})
            data["meta"].setdefault("education_title", q_meta.get("education_title", "Eƒüitim"))
            data["meta"]["num_answers"] = len([a for a in answers if a.get("answer")])
            data["meta"]["language"] = language

            st.session_state["analysis_data"] = data
            st.session_state["analysis_text"] = None

with c2:
    if st.session_state.get("analysis_data"):
        if st.button("‚¨áÔ∏è JSON indir", use_container_width=True):
            st.download_button(
                "ƒ∞ndir (analysis.json)",
                data=json.dumps(st.session_state["analysis_data"], ensure_ascii=False, indent=2),
                file_name="analysis.json",
                mime="application/json",
            )
with c3:
    pass

# ------------------------------
# Show output
# ------------------------------
if st.session_state.get("analysis_data"):
    st.markdown("---")
    st.subheader("üìé Analiz Sonucu")
    data = st.session_state["analysis_data"]

    # Validate (best-effort)
    try:
        jsonschema_validate(data, ANALYSIS_SCHEMA)
    except Exception as e:
        st.warning(f"≈ûema doƒürulamasƒ± uyarƒ±sƒ±: {e}")

    # Render
    meta = data.get("meta", {})
    st.write(f"**Eƒüitim:** {meta.get('education_title', 'Eƒüitim')} ¬∑ **Yanƒ±t sayƒ±sƒ±:** {meta.get('num_answers', 0)}")

    cols = st.columns(3)
    with cols[0]:
        st.markdown("**Temalar**")
        for t in data.get("themes", []):
            st.write("‚Ä¢", t)
    with cols[1]:
        st.markdown("**G√º√ßl√º Alanlar**")
        for t in data.get("strengths", []):
            st.write("‚Ä¢", t)
    with cols[2]:
        st.markdown("**Geli≈üim Alanlarƒ±**")
        for t in data.get("growth_areas", []):
            st.write("‚Ä¢", t)

    st.markdown("**Mikro Eylemler (√∂neri)**")
    for i, a in enumerate(data.get("micro_actions", []), start=1):
        st.write(f"{i}. {a}")

    st.markdown("**GA √úslubunda Anlatƒ±**")
    st.text_area("Anlatƒ±", value=data.get("ga_style_narrative", ""), height=300)

    if data.get("safety_notes"):
        st.info(data.get("safety_notes"))

    # Exports
    export_cols = st.columns(2)
    with export_cols[0]:
        pretty = json.dumps(data, ensure_ascii=False, indent=2)
        st.download_button("üì• analysis.json", data=pretty, file_name="analysis.json", mime="application/json")
    with export_cols[1]:
        md = [
            f"# {meta.get('education_title','Eƒüitim')} ‚Äì Ki≈üisel Analiz",
            "## Temalar",
            *[f"- {t}" for t in data.get("themes", [])],
            "\n## G√º√ßl√º Alanlar",
            *[f"- {t}" for t in data.get("strengths", [])],
            "\n## Geli≈üim Alanlarƒ±",
            *[f"- {t}" for t in data.get("growth_areas", [])],
            "\n## Mikro Eylemler",
            *[f"- {t}" for t in data.get("micro_actions", [])],
            "\n## GA √úslubunda Anlatƒ±\n",
            data.get("ga_style_narrative", ""),
            "\n\n---\nOtomatik √ºretildi: Hiperaktivist Analiz Sistemi",
        ]
        md_text = "\n".join(md)
        st.download_button("üìù Markdown indir", data=md_text, file_name="analysis.md", mime="text/markdown")

# ------------------------------
# Footer
# ------------------------------
st.markdown(
    """
---
**Kullanƒ±m Akƒ±≈üƒ±:**  
1) ƒ∞√ß sistemden √ºrettiƒüiniz `sorular_*.json` dosyasƒ±nƒ± y√ºkleyin.  
2) Eƒüitime ait `Eƒüitim` ve `Teknik & Y√∂ntemler` dosyalarƒ±nƒ± y√ºkleyin.  
3) Kullanƒ±cƒ± yanƒ±tlarƒ±nƒ± girin / yapƒ±≈ütƒ±rƒ±n.  
4) **Analizi √úret** d√ºƒümesine tƒ±klayƒ±n; JSON ve Markdown √ßƒ±ktƒ±larƒ± indirin.

**Notlar**  
‚Ä¢ √áƒ±ktƒ± GA √ºslubuna ve dok√ºmanlarƒ±nƒ±za sadƒ±k kalarak √ºretilir.  
‚Ä¢ JSON ≈üemasƒ± sayesinde raporlarƒ±nƒ±z tutarlƒ± yapƒ±dadƒ±r.  
‚Ä¢ Gerekirse `analysis.json` i√ßinden m√º≈üteri raporu PDF‚Äôleri √ºretebilirsiniz (ayrƒ± bir adƒ±mda).
"""
)
